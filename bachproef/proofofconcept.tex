%%=============================================================================
%% Proof-of-Concept
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Proof-of-Concept}{Proof-of-Concept}}%
\label{ch:ProofOfConcept}

Nu de bestaande tools en technieken met betrekking tot duplicaat detectie systemen en hun functionaliteiten besproken zijn, kan het proof-of-concept uitgewerkt worden. Er zal een duplicaat detectie systeem ontworpen worden die gebruik maakt van Machine Learning technieken.
\\Het prototype gaat in verschillende stappen te werk. Ten eerste zal een dataset van 5 000 records van artikel data verwerkt en klaargemaakt worden om het model te trainen. Daarna zal het model deze records opdelen in verschillende grote groepen die gelijkenissen met elkaar voortonen om zo makkelijker en sneller duplicaten te detecteren. Als laatste stap zal dan het effectieve duplicaat detectie systeem opgezet worden en zullen de duplicaten geanalyseerd worden.

\section{\IfLanguageName{dutch}{Data preparatie}{Data preparation}}%
\label{sec:dataPreparatiePOC}

De eerste stap van de voorbereidingsfase bestaat eruit om de data die zich in Excel bevindt in te lezen in Python en om te zetten naar een pandas dataframe. Dit wordt gedaan met behulp van de pandas library. Dit is nodig omdat pandas een krachtige library biedt voor data-analyse en manipulatie in Python. Het proces begint met het definiëren van het pad naar het Excel-bestand en de namen van de tabbladen die moeten worden ingelezen. In het tabblad 'MARA - Basisgegevens' bevinden zich alle basisgegevens van de artikels, zoals het bruto- en nettogewicht en het volume . In het tabblad 'MAKT - Omschrijving' bevinden zich de namen en de omschrijvingen van de artikelen en in het tabblad 'MBEW - Boekhouding' bevinden zich de boekhoudkundige gegevens van de artikelen, zoals de prijs. Vervolgens wordt het Excel-bestand ingelezen en worden de gegevens van de verschillende tabbladen in aparte dataframes opgeslagen.
\begin{lstlisting}[language=Python, caption={Het inlezen van Excel-gegevens in een pandas dataframe}]
  import pandas as pd
  
  # Define the file path and sheet names
  excel_file_path = 'ExcelPOC.xlsx'
  sheet_names = ['MARA - Basisgegevens', 'MAKT - Omschrijving', 'MBEW - Boekhouding']

  # Read the Excel file
  excel_file = pd.ExcelFile(excel_file_path)

  # Read the sheets
  sheets = {}
  for sheet_name in sheet_names:
      sheets[sheet_name] = excel_file.parse(sheet_name, index_col=None)
\end{lstlisting}

Vervolgens worden de dataframes die uit de verschillende tabbladen zijn ingelezen, samengevoegd. Dit gebeurt door middel van een outer join op de kolom 'Bron-ID', zodat alle gegevens behouden blijven, zelfs als sommige records niet in alle tabbladen voorkomen. Het gebruik van een outer join is belangrijk omdat dit ervoor zorgt dat geen data verloren gaat. Bij een inner join zouden alleen de records die in alle tabbladen voorkomen worden behouden, wat mogelijk waardevolle informatie zou verwijderen. De kolom ‘Bron-id’ bevat het id van de artikel data, daardoor moeten de gegevens aan de hand van deze kolom geordend worden.
\begin{lstlisting}[language=Python, caption={Het mergen van twee dataframes}]
  # Merge the sheets
  bap = sheets[sheet_names[0]]
  for sheet_name in sheet_names[1:]:
    bap = bap.merge(sheets[sheet_name], how='outer', on='Bron-ID')
\end{lstlisting}

Tot slot wordt de benodigde data geselecteerd. In dit geval betreft het de kolommen 'Artikelomschrijving', 'Artikelomschrijving.1' en 'Voorts.gem.prijs'. Er wordt gecontroleerd of de verwachte kolommen aanwezig zijn in het dataframe. Vervolgens worden de waarden omgezet naar strings en worden de eventuele ontbrekende waarden aangevuld met lege strings. Dit helpt om syntaxfouten te vermijden en zorgt ervoor dat de data correct kan worden verwerkt.
\begin{lstlisting}[language=Python, caption={Het selecteren en cleanen van de data}]
  # Select and clean the data if columns exist
  expected_columns = ['Artikelomschrijving', 'Artikelomschrijving.1', 'Voorts.gem.prijs']
  if all(col in bap.columns for col in expected_columns):
    data = bap[expected_columns]
    data = data.astype(str).fillna('')
  else:
    data = pd.DataFrame(columns=expected_columns).astype(str)
\end{lstlisting}

\section{\IfLanguageName{dutch}{Input data voorbereiding}{Input data preperation}}%
\label{sec:inputPreparatiePOC}

In de volgende fase wordt een dictionary met input velden opgezet. Deze dictionary wordt later gebruikt om de data voor te bereiden voor verdere verwerking. Er wordt gebruik gemaakt van TensorFlow's tf.keras.Input om de velden te definiëren. Door elk veld als een aparte input te behandelen, kan er flexibeler omgegaan worden met de data en specifieke preprocessing technieken toepassen op verschillende kolommen.
\begin{lstlisting}[language=Python, caption={Opzetten van een dictionary}]
  import tensorflow as tf
  from tensorflow import keras

  # Create a dictionary to define the input layers
  inputs = {}
  for name, column in data.items():
    inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=tf.string)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Uitkomst van de dictionary}]
  {'Artikelomschrijving': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=Artikelomschrijving>, 'Artikelomschrijving.1': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=Artikelomschrijving.1>, 'Voorts.gem.prijs': <KerasTensor shape=(None, 1), dtype=string, sparse=None, name=Voorts.gem.prijs>}
\end{lstlisting}

De tekstvelden in de data moeten worden omgezet naar numerieke waarden. Dit gebeurt met behulp van de StringLookup en CategoryEncoding methoden uit de tensorflow.keras.layers module. Deze methoden zorgen ervoor dat tekstgegevens kunnen worden omgezet naar een numeriek formaat dat door het model kan worden verwerkt. StringLookup zet elke unieke string om in een numerieke index. Vervolgens gebruikt CategoryEncoding deze indices om one-hot encoding toe te passen. One-hot encoding zet de indices om in binaire vectoren, wat helpt om de categorieën te representeren op een manier die door het model kan worden begrepen.
\begin{lstlisting}[language=Python, caption={Omzetten van tekstvelden naar numerieke waarden}]
  from tensorflow.keras.layers import StringLookup, CategoryEncoding
  
  # Preprocess the input data using StringLookup and CategoryEncoding
  encoding_dim1 = 32
  encoding_dim2 = 16
  preprocessed_inputs = []
  for name, input in inputs.items():
    lookup = StringLookup(vocabulary=np.unique(data[name]))
    one_hot = CategoryEncoding(num_tokens=lookup.vocabulary_size())
    x = lookup(input)
    x = one_hot(x)
    preprocessed_inputs.append(x)
\end{lstlisting}


\section{\IfLanguageName{dutch}{Autoencoder model}{Autoencoder model}}%
\label{sec:AutoencoderModelPOC}

Een autoencoder model wordt opgezet om de data te reduceren. Een autoencoder is een type neuraal netwerk dat is ontworpen om data te comprimeren en vervolgens weer te reconstrueren. Het bestaat uit een encoder die de data comprimeert en een decoder die de data reconstrueert.
\\We definiëren de encoder- en decoder-lagen met Dense-lagen en ReLU-activeringsfuncties. De encoder comprimeert de data naar een lagere dimensie, terwijl de decoder de data probeert terug te brengen naar de oorspronkelijke dimensie. Dit proces helpt om de belangrijkste kenmerken van de data te extraheren en irrelevante informatie te verminderen.
\begin{lstlisting}[language=Python, caption={Opzetten van het autoencoder model}]
  from tensorflow import keras
  from tensorflow.keras.layers import Input, Dense

  # Define the encoding layers
  encoded_layer1 = keras.layers.Dense(encoding_dim1, activation='relu')(preprocessed_inputs[0])
  encoded_layer2 = keras.layers.Dense(encoding_dim2, activation='relu')(encoded_layer1)

  # Define the decoding layers
  decoded_layer3 = keras.layers.Dense(encoding_dim2, activation='relu')(encoded_layer2)
  decoded_layer2 = keras.layers.Dense(encoding_dim1, activation='relu')(decoded_layer3)

  # Build the model
  model = keras.Model(inputs=inputs, outputs=decoded_layer2)
\end{lstlisting}


Na het definiëren van het model worden de input data voorbereid voor voorspelling en wordt het model getraind om de data te comprimeren. De data worden omgezet in een lijst van kolommen, zodat het model elke kolom afzonderlijk kan verwerken. Vervolgens wordt het model gebruikt om de data te comprimeren, wat resulteert in een gereduceerde dataset die klaar is voor clustering.
\begin{lstlisting}[language=Python, caption={Trainen van het model}]
  # List input data for each input layer
  input_data_list = [data[col] for col in data.columns]

  # Predict with the model
  encoded_items = model.predict(input_data_list)
\end{lstlisting}

\section{\IfLanguageName{dutch}{Clustering}{Clustering}}%
\label{sec:clusteringPOC}

Met de gereduceerde data wordt vervolgens K-Means clustering uitgevoerd. Deze techniek verdeelt de data in clusters op basis van gelijkenissen tussen de records. K-Means verdeelt de data in "n-clusters" clusters. Door de data in clusters te verdelen, kunnen er records gegroepeerd worden die vergelijkbaar zijn. Dit is nuttig voor het identificeren van patronen en mogelijke duplicaten in de data. e resultaten hiervan worden opgeslagen in ‘P’. ‘P’ bevat nu een lijst die bestaat uit een nummer van 0 tot en met 2 van de cluster waartoe elk data record behoort. Er wordt gekozen voor 3 clusters omdat in de output van de input dictionary er 3 attributen als input gedefinieerd zijn.
\begin{lstlisting}[language=Python, caption={Uitvoeren van K-Means clustering}]
  from sklearn.cluster import KMeans
  
  # Perform KMeans clustering
  kmeans = KMeans(init='k-means++', n_clusters=3, n_init=10)
  kmeans.fit(encoded_items)
  P = kmeans.predict(encoded_items)
\end{lstlisting}  


\section{\IfLanguageName{dutch}{Duplicaat detectie}{Duplicat Detection}}%
\label{sec:duplicateeDetectiePOC}

De laatste stap in dit proof-of-concept is het detecteren van mogelijke duplicaten binnen elk cluster. Dit wordt gedaan met behulp van de dedupe library, die speciaal is ontworpen voor het vinden van duplicaten in datasets. Dedupe maakt gebruik van een combinatie van machine learning en fuzzy matching technieken om overeenkomende records te identificeren.
\\Als eerste worden de velden die we willen gebruiken voor de duplicaatdetectie opgezet. Deze velden worden gespecificeerd als een lijst van dictionaries, waarbij elke dictionary de naam van het veld en het type (bijv. String) bevat.
\\Daarna wordt er een Dedupe instance opgezet en wordt deze voorbereidt op een training met de dataset. De dataframe wordt geconverteerd naar een dictionary van lijsten, omdat Dedupe deze indeling verwacht.
\begin{lstlisting}[language=Python, caption={Opzetten van een Dedupe instance en trainen van deze instance met de data}]
  import dedupe

  # Sample data
  fields = [
      {'field': 'Artikelomschrijving', 'type': 'String'},
      {'field': 'Artikelomschrijving.1', 'type': 'String'},
      {'field': 'Voorts.gem.prijs', 'type': 'String'}
  ]

  # Use the dedupe library for fuzzy matching
  def dedupe_dataframe(dataframe, fields):
    deduper = dedupe.Dedupe(fields)

    # Convert dataframe to a dictionary of lists
    data = {field['field']: dataframe[field['field']].tolist() for field in fields}

    # Check if field names in data match with expected fields
    if set(data.keys()) != set([field['field'] for field in fields]):
      print("Field names in data do not match with expected fields.")
      return []

    # Prepare the Dedupe instance based on data and clustering
    try:
      deduper.prepare_training(data)
    except Exception as e:
      print("Error while preparing Dedupe instance:", e)
      return []

    # Give labels to the selected training data using the consoleLabel method
    dedupe.training.consoleLabel(deduper)

    # Train the Dedupe instance
    deduper.train()

    # Apply a threshold value to find matches
    threshold = 0.5

    # Find matches with the get_matches() method
    matches = deduper.match(data, threshold=threshold)

    return matches

  # Perform fuzzy matching using the Dedupe_dataframe
  clustered_dupes = dedupe_dataframe(data, fields)
\end{lstlisting}

De gedetecteerde duplicaten worden geanalyseerd en de resultaten worden opgeslagen in een DataFrame. Dit helpt bij het identificeren van mogelijke duplicaten in de dataset. 
\\Na het trainen van de Dedupe instance en het uitvoeren van de matching, worden de resultaten geanalyseerd. De gedetecteerde duplicaten worden geplaatst in een gestructureerd formaat en worden opgeslagen in een dataframe voor verdere analyse. Voor elk cluster worden de records geïdentificeerd  die mogelijk duplicaten zijn en worden deze opgeslagen met hun respectieve scores. Dit helpt bij het bepalen van de nauwkeurigheid en effectiviteit van de deduplicatie.
\begin{lstlisting}[language=Python, caption={Identificeren van mogelijke duplicaten in de dataset}]
  # Analyze result and show dummy data
  results = []
  for cluster_id, (cluster, score) in enumerate(clustered_dupes):
    for record_id in cluster:
      results.append({
        'cluster_id': cluster_id,
        'record_id': record_id,
        'score': score
      })

  results_df = pd.DataFrame(results)
  print(results_df.head())
\end{lstlisting}

\section{\IfLanguageName{dutch}{Analyseren van de resultaten }{Analysis of the results}}%
\label{sec:analyseResultatenPOC}

\begin{lstlisting}[language=Python, caption={Output dedupe dataframe}]
  id	                Artikelomschrijving		 Artikelomschrijving.1 	                                      Voorts.gem.prijs	cluster_id	record_id		score
  000000000000405000  Playstation 5          Playstation 5	                                              499,00	          1	          255	         0.50913
  000000000000405020  Playstadion 5          Playstadion 5	                                              499,00	          1	          345	         0.50913
  000000000000406240  Samsung Televisie      Samsung UE43CU7190 - 43 inch - 4K LED - 2023                 589,00	          2	          726	         0.34925
  000000000000407620  Samsung Televisie      Samsung UE75CU7170UXXN - LED - 75- 4K Ultra HD - Smart TV  599,00	          2	          27	         0.34259
\end{lstlisting}

Als output van deze code, volgt een dataset waarin de oorpronkelijke data is uitgebreid met 'cluster-id', 'record-id' en 'score'. De 'cluster-id' bevat een nummer van 0 tot en met 2 die aangeeft tot welke cluster het record behoort. Het 'record-id' geeft het id van het record terug en de 'score' geeft aan hoe groot de score is dat er een match zich bevindt in de dataset.
Vanwege GDPR redenen is de effectieve data gemaskeerd  en vervangen door dummy data. In de output van de dedupe dataframe kunnen we zien dat 'Playstadion 5' als duplicaat wordt gezien van 'Playstation 5'. Ze verschillen op vlak van de schrijfwijze, maar ze zijn eigenlijk hetzelfde artikel. Hierdoor wordt er voldaan aan onze eerste requirement, het herkennen van schrijffouten en schrijfwijzen. Het artikel 'Samsung Televisie' wordt ook vermeld als duplicaat, maar als er dieper gekeken wordt naar de artikelomschrijving.1, kunnen we zien dat het hier eigenlijk gaat om twee verschillende televisies. Hierdoor meldt het duplicaat detectie systeem een vals positief duplicaat.
\\Deze proof-of-concept biedt al basis waarmee duplicaten gedetecteerd kunnen worden in een bestaande dataset. Het kan ook op nieuwe data uitgevoerd worden. Daarnaast is het model ook ontworpen zodat het kan blijven leren van andere data en zo het model geoptimaliseerd kan worden. Het verder onderzoeken en testen van nieuwe algoritmen op vlak van clustering en duplicaat detectie kan voor nog betere resultaten zorgen. 
\\ \\Vooraleer het prototype ontwerpen werd, zijn er verschillende requirements opgesteld waaraan een goed duplicaat detectie systeem moet voldoen.
\\Ten eerste is het in staat om schrijffouten en variaties in schrijfwijze te detecteren en zo duplicaten te herkennen. Ook pas de standaardisatie toe om duplicaten te kunnen identificeren.
\\Echter, kan het model nog geen fonetische gelijkenissen herkennen. Dit is een mogelijke verbetering die kan worden toegevoegd aan het model. Dit zou mogelijks kunnen verbeterd worden door ene geschikte Python library toe te voegen die deze functionaliteit ondersteunt. Eveneens kunnen ook andere fuzzy matching algoritmen, die alternatieve methoden gebruiken in plaats van Affine Gap Penalty, geïntegreerd worden.
\\Speciale tekens en karakters worden momenteel nog niet door het model verwerkt.
\\Door matching-algoritmen kan de data verwerkt worden en een score toegevoegd worden aan mogelijkse matches. Als er meerdere algoritmen worden toegevoegd aan het model, kan er beslist worden om het ene algoritme zwaarder door te laten wegen dan het andere. 