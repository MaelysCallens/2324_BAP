%---------- Inleiding ---------------------------------------------------------

\section{Introductie}%
\label{sec:introductie}

Bepaalde bedrijven hebben nog steeds moeite met het ordenen van hun artikel data. De master gegevens worden geïdentificeerd als de kerngegevens van een organisatie. Ze bevatten gegevens over klanten, leveranciers, producten, enz. Door het gebrek van de kwaliteit aan het bijhouden en het managen van deze gegevens hebben grote bedrijven hier vaak moeilijkheden mee. Deze moeilijkheden kunnen vaak grote risico’s met zich meebrengen, zoals het betalen van hoge kosten om alles weer op punt te laten zetten. 

In deze bachelorproef zal er nagegaan worden hoe Machine Learning/Artificial Intelligence de kwaliteit van artikel data kan verhogen. Deze paper zal antwoorden bieden op volgende deelvragen: Hoe wordt beoordeeld of de kwaliteit van artikel data verhoogd is?; Welke methoden zijn momenteel al aanwezig op de markt?; Wat zijn de vereisten voor het opzetten van zo een eigen tool? Vanuit de resultaten van het onderzoek en de ondervindingen, zal er een proof-of-concept opgesteld worden die aan de hand van ML/AI de kwaliteit van artikel data kan verhogen. Dit wordt bereikt door gebruik te maken van datasets die verzameld worden. Wanneer nieuwe artikel data worden ingevoerd in de tool, zullen er specifieke voorstellen worden gedaan die passen binnen dat veld, deze kunnen dan vervolgens al dan niet worden goedgekeurd.

%---------- Stand van zaken ---------------------------------------------------

\section{State-of-the-art}%
\label{sec:state-of-the-art}

% Voor literatuurverwijzingen zijn er twee belangrijke commando's:
% \autocite{KEY} => (Auteur, jaartal) Gebruik dit als de naam van de auteur
%   geen onderdeel is van de zin.
% \textcite{KEY} => Auteur (jaartal)  Gebruik dit als de auteursnaam wel een
%   functie heeft in de zin (bv. ``Uit onderzoek door Doll & Hill (1954) bleek
%   ...'')

De meeste bedrijven gebruiken tegenwoordig honderden verschillende applicaties en systemen die verschillende afdelingen doorkruisen. Bijvoorbeeld: Enterpise Resource Planning (ERP), Human Capital Management (HCM) en Customer Relationship Management (CRM). Omdat zoveel mensen de data in deze systemen aanraken, is het daardoor gemakkelijker om geïsoleerde, dubbele, verouderde of zelfs tegenstrijdige data te hebben. Slechte gegevens leiden tot slechte besluitvormingen, hierbij is het dus belangrijk om te voldoen aan de behoefte van tijdige, accurate informatie, zelfs als de gegevensbronnen toenemen, stappen bedrijven over op master data management (MDM) \autocite{SAP}.

Master Data Management is een belangrijke methode voor het ontwikkelen en het behouden van uniformiteit en nauwkeurigheid van gedeelde master data van de organisatie. Met Master Data Governance kunnen bedrijven de uniformiteit en de nauwkeurigheid van hun belangrijke gegevensmiddelen verbeteren, zoals klantengegevens, productengegevens, activagegevens en locatiegegevens \autocite{Foote2023}.

Master data gegevens worden omschreven als de kerngegevens van een organisatie. Ze zijn gebaseerd op informatie die zelden veranderen en die essentieel zijn voor het runnen van de bedrijfsactiviteiten. Bedrijven verzamelen en slaan steeds meer gegevens op over hun producten, gegevensmiddelen, inventaris en klanten. Deze moeten dan beheerd blijven om accuraat te zijn. Als er onnauwkeurige master data aanwezig is in het bedrijf, is het vaak moeilijker om intelligente beslissingen over het bedrijf te maken. 

Voor de meeste bedrijvenactiviteiten zijn informatiesystemen (IS) belangrijke factoren voor de bedrijfsuitvoering en de besluitvorming. Volgens het onderzoek van \textcite{Knolmayer2006} toont aan dat ERP-systemen worden beschouwd als het centrale onderdeel van het huidige IS-landschap. Het biedt organisaties grote applicatiefunctionaliteit, die een groot deel van alle bedrijfsactiviteiten ondersteunt. Hun ontwerp belooft het probleem van gefragmenteerde informatie in grote organisaties op te lossen. Dit aan de hand van verschillende soorten gegevens uit zeer verschillende bedrijfsactiviteiten, zoals verkoop, personeelszaken, enz. samen te brengen in een consistent bedrijfsmodel. De distributie van master data wordt vaak nagegaan door een proces dat ervoor moet zorgen dat alle data wordt ingevoerd en goedgekeurd met inachtneming van de bedrijfsregels. Ook moet elke gebruiker en elk systeem nieuwe of bijgewerkte artikel data ontvangen zodra dat nodig is. Daardoor is het essentieel dat de kwaliteit van master data zo hoog mogelijk is. 

Door de technologische ontwikkelingen is men ertoe geleid dat bedrijven steeds meer data opslaan. Het onderhoud van de gegevenskwaliteit wordt echter vaak verwaarloosd, daardoor kan het voor sommige bedrijven zijn dat de slechte kwaliteit van de bedrijfsgegevens hoge kosten met zich meebrengt. Het artikel van \textcite{Haug2011} toont aan dat een perfecte datakwaliteit niet vereist is, maar dat de datakwaliteit tot aan een bepaald niveau moet worden verbeterd. Het identificeren van het optimale gegevenskwaliteitsniveau is hierbij noodzakelijk.

\textcite{ Haug2011a} hebben een onderzoek uitgevoerd naar de belemmeringen voor het beheersen van de datakwaliteit in bedrijven. De eindresultaten van hun onderzoek geven aan dat een gebrek aan delegatie van verantwoordelijkheden voor het onderhouden van masterdata het enige aspect is dat de grootste impact heeft op de kwaliteit van master data. Ook blijkt dat de overgrote meerderheid van de bedrijven een standpunt heeft dat een slechte master datakwaliteit negatieve gevolgen heeft. 

%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

Voor het onderzoek naar hoe ML/AI kan gebruikt worden om de kwaliteit van artikel data te verhogen, is een plan van aanpak opgesteld. Dit plan van aanpak is opgesteld in verschillende fases. 

In de eerste fase is het doel om een diepgaande kennis te verwerven over het onderwerp. In de eerste deelfase zal er onderzoek gedaan worden naar relevante onderzoek artikelen, academische papers en boeken die master data verwoorden, de mogelijkse technieken die gebruikt worden om de kwaliteit van master data te verhogen en ook de technologieën die werken aan de hand van ML/AI om de kwaliteit te verhogen. Daarna zullen de technieken en de soort gelijke oplossingen die in de eerdere onderzoeken zijn vernomen onderzocht worden. 

Na de eerste fase zullen er duidelijke doelstellingen bepaald worden voor het verdere verloop van het onderzoek. In deze fase zullen de verschillende technieken en mogelijke oplossingen worden vergeleken met elkaar en zal er nagegaan worden of deze oplossingen rendabel genoeg zijn om zo een tool zelf te ontwerpen en op de markt te brengen. 

Na deze fase zal de techniek of oplossing gekozen worden die dit kan verwezenlijken. Deze techniek of oplossing zal geselecteerd worden aan de hand van de doelstellingen die in voorgaande fases zijn vastgelegd. Tijdens deze fase zal er onderzocht worden welk neuraal netwerk we nodig hebben en of er al dan niet bestaande bibliotheken aanwezig zijn die we kunnen gebruiken en welke de beste is. 

In de volgende fase zal er een dataset verzameld worden. Het verzamelen van een representatieve dataset is essentieel voor het succes van het onderzoek. Er zal opzoek moeten gegaan worden naar een dataset met een brede scala aan artikel data zodat de kwaliteit van de oplossing kan getest worden. Om een goede dataset te vinden zal deze samengesteld worden aan de hand van datasets van specifieke bedrijven die veel gebruik maken van artikel data.

In de vijfde fase zal er een proof-of-concept uitgewerkt worden. Gebaseerd op de literatuurstudie en het onderzoek zal er een proof-of-concept ontworpen worden met ML/AI die de kwaliteit van master data mogelijks kan verhogen. De nodige middelen zullen afhankelijk zijn van welke oplossing er uit het onderzoek is gekomen. Het model zal getraind worden aan de hand van de verzamelde dataset. De proof-of-concept zal opgesteld worden zodanig dat als men iets wil ingeven er een lijst van mogelijke invoeringen voor dat veld te voorschijn komt zodat er minder dubbele en tegenstrijdige gegevens ingevoerd kunnen worden. Ook zal er onderzocht worden hoe het model continu kan blijven leren van nieuwe artikels die aangemaakt worden. Zodat als er nieuwe artikels ingevoerd worden de mogelijkse veelkomende invoeringen ter beschikking gesteld worden.

Na de uitwerking van de proof-of-concept zal er onderzoek verricht worden naar hoe deze oplossing makkelijk in de SAP Master Data Governance (MDG) tool kan geïntegreerd worden. Er zal gekeken worden naar waar en hoe dit best geïmplementeerd kan worden om de uitwerking ervan optimaal te kunnen benutten door bijvoorbeeld met behulp van een API.

Als laatste zal er een conclusie worden getrokken uit het onderzoek en een antwoord worden geformuleerd op de onderzoeksvraag op basis van de verkregen resultaten. Er zal ook een aanbeveling geschreven worden voor de implementatie van het meest efficiënte en rendabelste model om de kwaliteit van artikel data te verhogen.

\begin{center}
  \includegraphics[scale=0.4]{../images/methodologie.png}
\end{center}

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

Door te onderzoeken welke technologieën en oplossingen er al op de markt aanwezig zijn in verband met master datakwaliteit kan er nagegaan worden of het al dan niet rendabel is om zelf een soortgelijke technologie op de markt te brengen. 

Het ontwikkelen van een nieuwe technologie en deze implementeren in SAP MDG Tool zal hoogstwaarschijnlijk wel rendabel zijn. Dit is voornamelijk omdat veel bedrijven het nog steeds moeilijk hebben met het ordenen en het managen van hun data. Dit is vaak doordat sommige data, zoals klantengegevens en productengegevens, door veel mensen gebruikt wordt en zo kunnen er verouderde gegevens of duplicaten ontstaan. Ook hebben nog niet veel IT-bedrijven een implementatie waarbij ML/AI de kwaliteit van artikel data kan verhogen. Hierdoor is er nog niet veel concurrentie aanwezig op de markt, wat het ook weer veel aantrekkelijker maakt om zoiets zelf te onderzoeken en te ontwerpen. 

Door de proof-of-concept op te bouwen kan er veel makkelijker nagegaan worden of de data al dan niet al aanwezig is in de dataset van het bedrijf en deze desnoods moet aangepast worden. Hierdoor zouden de bedrijven veel minder duplicaten of verouderde gegevens bezitten in hun dataset en zal dit geen grote kosten met zich meebrengen omdat de verwaarloosde dataset weer op punt gesteld moet worden.

